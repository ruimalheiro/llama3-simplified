{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import bitsandbytes as bnb\n",
    "import textwrap\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "from transformers import PretrainedConfig\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
    "from safetensors.torch import load_file\n",
    "from dataclasses import dataclass, asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clean_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, path):\n",
    "        self.special_tokens = defaultdict(int)\n",
    "        self.num_reserved_special_tokens = 256\n",
    "        self.pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n",
    "\n",
    "        self.vocab = load_tiktoken_bpe(path)\n",
    "        self.num_base_tokens = len(self.vocab)\n",
    "\n",
    "        special_tokens = [\n",
    "            '<|begin_of_text|>',\n",
    "            '<|end_of_text|>',\n",
    "            '<|reserved_special_token_0|>',\n",
    "            '<|reserved_special_token_1|>',\n",
    "            '<|reserved_special_token_2|>',\n",
    "            '<|reserved_special_token_3|>',\n",
    "            '<|start_header_id|>',\n",
    "            '<|end_header_id|>',\n",
    "            '<|reserved_special_token_4|>',\n",
    "            '<|eot_id|>',\n",
    "        ]\n",
    "\n",
    "        special_tokens += [\n",
    "            f'<|reserved_special_token_{i}|>'\n",
    "            for i in range(5, self.num_reserved_special_tokens - 5)\n",
    "        ]\n",
    "\n",
    "        self.special_tokens = {\n",
    "            token: self.num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(path).name,\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=self.vocab,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "\n",
    "        self.number_of_words = self.model.n_vocab\n",
    "\n",
    "        self.bos_id = self.special_tokens['<|begin_of_text|>']\n",
    "        self.eos_id = self.special_tokens['<|end_of_text|>']\n",
    "        self.pad_id = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens['<|end_of_text|>'],\n",
    "            self.special_tokens['<|eot_id|>'],\n",
    "        }\n",
    "        \n",
    "    def encode(\n",
    "        self,\n",
    "        s,\n",
    "        *,\n",
    "        bos,\n",
    "        eos,\n",
    "        allowed_special=set(),\n",
    "        disallowed_special=()\n",
    "    ):\n",
    "        tokens = self.model.encode(s, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "        if bos:\n",
    "            tokens.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            tokens.append(self.eos_id)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, t):\n",
    "        return self.model.decode(t)\n",
    "\n",
    "    def encode_with_prompt(self, text):\n",
    "        prompt = f'''<|begin_of_text|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{text}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "        return self.encode(prompt, bos=True, eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('./tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dim': 4096,\n",
       " 'n_layers': 32,\n",
       " 'n_heads': 32,\n",
       " 'n_kv_heads': 8,\n",
       " 'vocab_size': 128256,\n",
       " 'multiple_of': 1024,\n",
       " 'ffn_dim_multiplier': 1.3,\n",
       " 'norm_eps': 1e-05,\n",
       " 'rope_theta': 500000.0,\n",
       " 'max_batch_size': 6,\n",
       " 'max_seq_len': 512}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./hyperparameters.json', 'r') as f:\n",
    "    hyperparameters = json.loads(f.read())\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a data class so it is easier to pass the properties within the model by accessing a global hparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({\n",
       "     \"dim\": 4096,\n",
       "     \"n_layers\": 32,\n",
       "     \"n_heads\": 32,\n",
       "     \"n_kv_heads\": 8,\n",
       "     \"vocab_size\": 128256,\n",
       "     \"multiple_of\": 1024,\n",
       "     \"ffn_dim_multiplier\": 1.3,\n",
       "     \"norm_eps\": 1e-05,\n",
       "     \"rope_theta\": 500000.0,\n",
       "     \"max_batch_size\": 6,\n",
       "     \"max_seq_len\": 512\n",
       " },\n",
       " {'dim': 4096,\n",
       "  'n_layers': 32,\n",
       "  'n_heads': 32,\n",
       "  'n_kv_heads': 8,\n",
       "  'vocab_size': 128256,\n",
       "  'multiple_of': 1024,\n",
       "  'ffn_dim_multiplier': 1.3,\n",
       "  'norm_eps': 1e-05,\n",
       "  'rope_theta': 500000.0,\n",
       "  'max_batch_size': 6,\n",
       "  'max_seq_len': 512})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int\n",
    "    vocab_size: int\n",
    "    multiple_of: int\n",
    "    ffn_dim_multiplier: float\n",
    "    norm_eps: float\n",
    "    rope_theta: float\n",
    "    max_batch_size: int\n",
    "    max_seq_len: int\n",
    "\n",
    "    def __repr__(self):\n",
    "        return json.dumps(asdict(self), indent=4)\n",
    "\n",
    "hparams = ModelConfig(\n",
    "    dim=4096,\n",
    "    n_layers=32,\n",
    "    n_heads=32,\n",
    "    n_kv_heads=8,\n",
    "    vocab_size=128256,\n",
    "    multiple_of=1024,\n",
    "    ffn_dim_multiplier=1.3,\n",
    "    norm_eps=1e-05,\n",
    "    rope_theta=500000.0,\n",
    "    max_batch_size=6,\n",
    "    max_seq_len=512\n",
    ")\n",
    "\n",
    "hparams, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_complex_exponential(dim, sequence_length, theta=10000.0):\n",
    "    ''' Computes the frequencies that will be used for positional encoding and also for rotary embeddings in\n",
    "    the attention mechanism.\n",
    "    '''\n",
    "    # Get the even indices within the embedding dimension and normalises them.\n",
    "    even_indices = torch.arange(0, dim, 2)[: (dim // 2)].float()\n",
    "    normalised_even_indices = even_indices / dim\n",
    "\n",
    "    # Formula for the frequencies.\n",
    "    freqs = 1.0 / (theta ** normalised_even_indices)\n",
    "\n",
    "    # Gets an increasing sequence to the size of the input sequence (time steps).\n",
    "    timesteps = torch.arange(sequence_length, device=freqs.device, dtype=torch.float32)\n",
    "\n",
    "    # Multiplies each timestep for all values in frequencies to form the frequencies matrix.\n",
    "    # These will be the angles for the polar function.\n",
    "    freqs = torch.outer(timesteps, freqs)\n",
    "\n",
    "    # Creates a mask filled with ones.\n",
    "    ones = torch.ones_like(freqs)\n",
    "\n",
    "    # Computes the complex tensor representing the cartesian coordinates that correspond to the polar coordinates (abs \"ones\" and angles \"freqs\").\n",
    "    freqs_complex_exponential = torch.polar(ones, freqs)\n",
    "\n",
    "    return freqs_complex_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis, ndim, shape):\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    ''' Apply the rotary embeddings.\n",
    "    '''\n",
    "    # We start by reshaping the inputs. Their last dimension is the head_dim, so we need to make sure we split the head dim into 2 parts\n",
    "    # to account for the complex part.\n",
    "    xq_complex = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_complex = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "\n",
    "    # Ensure freqs_cis has the correct dimensions compatible with broadcasting. E.g (a, 1, b, c, 1)\n",
    "    # Note that xq has shape (batch_size, sequence_length, n_heads, head_dim), meaning with the hyperparamers as they are\n",
    "    # it will be (6, 512, 32, 4096 / 32) -> (6, 512, 32, 128). xq_complex will be (6, 512, 32, 64, 2).\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_complex.ndim, xq_complex.shape)\n",
    "\n",
    "    # Now we can apply the rotary embeddings and flatten from dimension 3 (so we get the 128 back with 4 dimensions instead of 5.\n",
    "    xq_out = torch.view_as_real(xq_complex * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_complex * freqs_cis).flatten(3)\n",
    "\n",
    "    # Retain the datatypes\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x, n_rep):\n",
    "    ''' Repeat x n_rep times. The idea is to spread these tensors through more attention heads. \n",
    "    '''\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "        \n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.n_heads = hparams.n_heads\n",
    "        self.n_kv_heads = hparams.n_heads if hparams.n_kv_heads is None else hparams.n_kv_heads\n",
    "        self.head_dim = hparams.dim // hparams.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(hparams.dim, hparams.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(hparams.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(hparams.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(hparams.n_heads * self.head_dim, hparams.dim, bias=False)\n",
    "\n",
    "        self.register_buffer('cache_k', torch.zeros((hparams.max_batch_size, hparams.max_seq_len, self.n_kv_heads, self.head_dim)))\n",
    "        self.register_buffer('cache_v', torch.zeros((hparams.max_batch_size, hparams.max_seq_len, self.n_kv_heads, self.head_dim)))\n",
    "\n",
    "    def forward(self, x, start_pos, freqs_cis, mask=None):\n",
    "        batch_size, sequence_length, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(batch_size, sequence_length, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, sequence_length, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, sequence_length, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:batch_size, start_pos : start_pos + sequence_length] = xk.clone().detach()\n",
    "        self.cache_v[:batch_size, start_pos : start_pos + sequence_length] = xv.clone().detach()\n",
    "\n",
    "        keys = self.cache_k[:batch_size, : start_pos + sequence_length]\n",
    "        values = self.cache_v[:batch_size, : start_pos + sequence_length]\n",
    "\n",
    "        keys = repeat_kv(keys, 4)\n",
    "        values = repeat_kv(values, 4)\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # if mask is not None:\n",
    "        #     scores = scores + mask\n",
    "        # scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # output = torch.matmul(scores, values)\n",
    "\n",
    "        output = F.scaled_dot_product_attention(xq, keys, values, scale=(1 / math.sqrt(self.head_dim)), attn_mask=mask)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, sequence_length, -1)\n",
    "\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, multiple_of, ffn_dim_multiplier):\n",
    "        super(FeedForward, self).__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.n_heads = hparams.n_heads\n",
    "        self.dim = hparams.dim\n",
    "        self.attention = Attention()\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=hparams.dim,\n",
    "            hidden_dim=4 * hparams.dim,\n",
    "            multiple_of=hparams.multiple_of,\n",
    "            ffn_dim_multiplier=hparams.ffn_dim_multiplier\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(hparams.dim, eps=hparams.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(hparams.dim, eps=hparams.norm_eps)\n",
    "\n",
    "    def forward(self, x, start_position, freqs_cis, mask=None):\n",
    "        hidden_state = x + self.attention(self.attention_norm(x), start_position, freqs_cis, mask)\n",
    "        output = hidden_state + self.feed_forward(self.ffn_norm(hidden_state))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required so it is compatible with the Hugging Face trainer.\n",
    "class CustomConfig(PretrainedConfig):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_type = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # required for Hugging Face\n",
    "        self.config = CustomConfig()\n",
    "\n",
    "        self.vocab_size = hparams.vocab_size\n",
    "        self.n_layers = hparams.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(\n",
    "            hparams.vocab_size,\n",
    "            hparams.dim\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_id in range(self.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id))\n",
    "\n",
    "        self.norm = RMSNorm(hparams.dim, eps=hparams.norm_eps)\n",
    "        self.output = nn.Linear(hparams.dim, hparams.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_complex_exponential(\n",
    "            hparams.dim // hparams.n_heads,\n",
    "            hparams.max_seq_len * 2,\n",
    "            hparams.rope_theta\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        start_position=0,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None\n",
    "    ):\n",
    "        ''' Some arguments in the method signature are not used but are required to make this compatible with the trainer from Hugging Face.\n",
    "        '''\n",
    "        batch_size, sequence_length = input_ids.shape\n",
    "        hidden_state = self.tok_embeddings(input_ids)\n",
    "        self.freqs_cis = self.freqs_cis.to(hidden_state.device)\n",
    "\n",
    "        freqs_cis = self.freqs_cis[start_position : start_position + sequence_length]\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        if sequence_length > 1:\n",
    "            mask = torch.full((sequence_length, sequence_length), float('-inf'), device=input_ids.device)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            cached_shift = torch.zeros((sequence_length, start_position), device = input_ids.device)\n",
    "            mask = torch.hstack([cached_shift, mask]).type_as(hidden_state) \n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_state = layer(hidden_state, start_position, freqs_cis, mask)\n",
    "\n",
    "        hidden_state = self.norm(hidden_state)\n",
    "\n",
    "        logits = self.output(hidden_state).float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            logits = logits.view(-1, logits.size(-1))  # (batch_size * sequence_length, num_classes)\n",
    "            labels = labels.view(-1)  # (batch_size * sequence_length)\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits, labels)\n",
    "\n",
    "        if loss is not None:\n",
    "            return {'logits': logits, 'loss': loss}\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    ''' Top P - Sorts the tokens from highest probabilities to lowest and calculates cumulative probabilities up to the cumulative >= p.\n",
    "    '''\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    \n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_and_top_p_sampling(logits, temperature, top_p):\n",
    "    ''' Applies temperature and calculates top P. If temperature is 0 we just get the token with highest logit.\n",
    "    '''\n",
    "    if temperature > 0:\n",
    "        probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "        next_token = sample_top_p(probs, top_p)\n",
    "    else:\n",
    "        next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt_tokens, max_gen_len, temperature, top_p, full_seq, token_window):\n",
    "    ''' Note: It is better to use full_seq = True for improved quality in the output.\n",
    "    This implementation uses a KV cache to speed up the generation. By default, after generating the first token, it needs only the latest token in order to take advantage of the cache.\n",
    "    It works well in 16/8-bit quantization, but I had a few issues when I quantized it further to 4-bit.\n",
    "    To address that, I created a 'token window' that works as a sliding window and provides enough context for the model.\n",
    "    '''\n",
    "    batch_size = len(prompt_tokens)\n",
    "\n",
    "    # Finding the boundaries / limits.\n",
    "    min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "    max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "    total_len = min(hparams.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "    # Here we assume we receive a batch of multiple tokenized sequences.\n",
    "    pad_id = tokenizer.pad_id\n",
    "    tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device='cuda')\n",
    "    \n",
    "    for batch, tokens_list in enumerate(prompt_tokens):\n",
    "        tokens[batch, : len(tokens_list)] = torch.tensor(tokens_list, dtype=torch.long, device='cuda')\n",
    "\n",
    "    # Define stop conditions, input mask and the stop tokens (extracted from the tokenizer)\n",
    "    previous_position = 0\n",
    "    eos_reached = torch.tensor([False] * batch_size, device='cuda')\n",
    "    input_text_mask = tokens != pad_id\n",
    "    stop_tokens = torch.tensor(list(tokenizer.stop_tokens), device='cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for current_position in range(min_prompt_len, total_len):\n",
    "            if full_seq:\n",
    "                # no cache\n",
    "                logits = model.forward(tokens[:, :current_position], start_position=0)\n",
    "            else:\n",
    "                # uses cache with a sliding window of size token_window.\n",
    "                logits = model.forward(tokens[:, max(0, previous_position - token_window):current_position], start_position=previous_position)\n",
    "                \n",
    "            # Temperature and sampling.\n",
    "            next_token = temperature_and_top_p_sampling(logits, temperature, top_p)\n",
    "            next_token = next_token.reshape(-1)\n",
    "\n",
    "            # Gets the next token depending on the condition (mask) and appends to tokens.\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, current_position], tokens[:, current_position], next_token\n",
    "            )\n",
    "            tokens[:, current_position] = next_token\n",
    "\n",
    "            # Checks if we reached the eos on all sequences in the batch and updates the current position.\n",
    "            eos_reached |= (~input_text_mask[:, current_position]) & (torch.isin(next_token, stop_tokens))\n",
    "            \n",
    "            previous_position = current_position\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        # For all the sequences, we extract all tokens up to a stop_token if it exists.\n",
    "        out_tokens = []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            start = len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "\n",
    "            for stop_token in tokenizer.stop_tokens:\n",
    "                try:\n",
    "                    eos_idx = toks.index(stop_token)\n",
    "                    toks = toks[:eos_idx]\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            out_tokens.append(toks)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return out_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dialogue_custom(\n",
    "    texts,\n",
    "    *,\n",
    "    max_gen_len=256,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    full_seq=False,\n",
    "    token_window=4,\n",
    "    text_width=200\n",
    "):\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "\n",
    "    prompt_tokens = [tokenizer.encode_with_prompt(text) for text in texts]\n",
    "    \n",
    "    generation_tokens = generate(\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        max_gen_len=max_gen_len,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        full_seq=full_seq,\n",
    "        token_window=token_window\n",
    "    )\n",
    "\n",
    "    results = [tokenizer.decode(t) for t in generation_tokens]\n",
    "\n",
    "    for result in results:\n",
    "        _result = result.split('<|eot_id|>')[0]\n",
    "\n",
    "        paragraphs = _result.split('\\n')\n",
    "        _result = '\\n'.join(textwrap.fill(paragraph, width=text_width) for paragraph in paragraphs)\n",
    "\n",
    "        print(_result)\n",
    "\n",
    "        if len(results) > 1:\n",
    "            print('\\n-------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and quantize the model with the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu()\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = Transformer()\n",
    "\n",
    "bnb_quantization_config = BnbQuantizationConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "\n",
    "model = load_and_quantize_model(\n",
    "    model,\n",
    "    bnb_quantization_config=bnb_quantization_config,\n",
    "    device_map = 'auto',\n",
    "    weights_location = './pretrained.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained weights tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "_header_id|>\n",
      "<|start_header_id|>assistant<\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom('Where is the city of new york?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "<\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom('What is healthier? Fish or Meat?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can add two numbers using the + operator in Python. For example, if you want to add 2 and 3, you can use the following code:\n",
      "result = 2 + 3\n",
      "print(result)\n",
      "This will print 5.\n",
      "Alternatively, you can use the sum() function to add a list of numbers. For example, if you have a list of numbers called numbers, you can use the following code to add all the numbers in the list:\n",
      "sum(numbers)\n",
      "This will return the sum of all the numbers in the list.\n",
      "You can also use the built-in function sum() to add a list of numbers. For example, if you have a list of numbers called numbers, you can use the following code to add all the numbers in the list:\n",
      "sum(numbers)\n",
      "This will return the sum of all the numbers in the list.\n",
      "Alternatively, you can use the built-in function sum() to add a list of numbers. For example, if you have a list of numbers called numbers, you can use the following code to add all the numbers in the\n",
      "list:\n",
      "sum(numbers)\n",
      "This will return the sum of all the numbers in the list.\n",
      "Alternatively, you can use the built-in function sum() to add a list of numbers. For example, if\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom('Using python, how do I write a function to add 2 numbers and print the result?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portugal's capital is Lisbon.\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "What is the capital of the UK?\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom(['What is the capital of Portugal?', 'What is the capital of the UK?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading finetuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(load_file('./finetuned.safetensors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuned weights tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city of New York is located in the state of New York in the United States of America. It is located in the northeastern part of the country, on the island of Manhattan. The city is situated on the\n",
      "Hudson River and is surrounded by the Atlantic Ocean on the east and the Long Island Sound on the north.\n",
      "The city is known for its iconic landmarks, including the Statue of Liberty, the Empire State Building, and the Brooklyn Bridge. It is also known for its diverse culture, with a large population of\n",
      "immigrants from around the world. The city is also home to a wide range of industries, including finance, media, and technology, and is a major hub for international trade and commerce.\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom('Where is the city of new york?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is difficult to say which is healthier, fish or meat. Both have their own advantages and disadvantages. Fish is a good source of protein and omega-3 fatty acids, which are beneficial for heart\n",
      "health. However, it is important to note that some types of fish may contain high levels of mercury, which can be harmful to health. Meat is also a good source of protein and nutrients, but it is\n",
      "important to choose lean cuts of meat and to limit consumption of processed meats, which are often high in saturated fat and sodium. In general, a balanced diet that includes both fish and meat is\n",
      "considered to be the most healthy option.\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom('What is healthier? Fish or Meat?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the `sum` function in Python to add two numbers and print the result. The `sum` function takes two arguments, the first being the list of numbers you want to add, and the second being the\n",
      "number you want to add to each number in the list. Here is an example of how to use the `sum` function:\n",
      "```\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "result = sum(numbers, 10)\n",
      "print(result)\n",
      "```\n",
      "In this example, the `sum` function takes the list `numbers` and adds 10 to each number in the list, resulting in the number 15. The `print` function is then used to print the result to the console.\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom('Using python, how do I write a program to add 2 numbers and print the result?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Portugal is Lisbon.\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "The capital of the UK is London.\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dialogue_custom(['What is the capital of Portugal?', 'What is the capital of the UK?'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ml_remote_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
